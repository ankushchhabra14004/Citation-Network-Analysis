{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a167f042",
   "metadata": {},
   "source": [
    "# Task 3 — PageRank with Varying Damping Factors\n",
    "This notebook applies the PageRank algorithm on the citation network (from `dblp_subset.json`) with damping factors ranging from 0.15 to 0.95 (step 0.10).\n",
    "\n",
    "For each damping factor:\n",
    "1. Compute PageRank scores for all papers.\n",
    "2. Select the top-50 papers by PageRank.\n",
    "3. Compute citation counts (in-degree) for these top-50 papers.\n",
    "4. Calculate Pearson correlation between PageRank scores and citation counts.\n",
    "\n",
    "Then report:\n",
    "- Correlation values for all damping factors.\n",
    "- Top-10 papers (with PageRank scores) for best and worst correlation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "427986a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10f49024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/ankushchhabra/Downloads/Data Mining Assignment2/dblp_subset.json\n",
      "Read 49572 papers in 0.87s\n",
      "Built mappings for 49572 papers\n",
      "Read 49572 papers in 0.87s\n",
      "Built mappings for 49572 papers\n"
     ]
    }
   ],
   "source": [
    "# Load the subset file and build the citation graph\n",
    "subset_path = os.path.join(os.getcwd(), 'dblp_subset.json')\n",
    "print('Reading', subset_path)\n",
    "start = time.time()\n",
    "papers = []\n",
    "with open(subset_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            papers.append(json.loads(line))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "n = len(papers)\n",
    "print(f'Read {n} papers in {time.time()-start:.2f}s')\n",
    "\n",
    "# Build mappings\n",
    "id_to_idx = {}\n",
    "titles = []\n",
    "refs_by_id = []\n",
    "for idx, p in enumerate(papers):\n",
    "    pid = p.get('id')\n",
    "    id_to_idx[pid] = idx\n",
    "    titles.append(p.get('title', '') or '')\n",
    "    refs_by_id.append(p.get('references', []) or [])\n",
    "\n",
    "print('Built mappings for', n, 'papers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7196cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building directed graph...\n",
      "Graph built in 0.38s\n",
      "Nodes: 49572, Edges: 163309\n"
     ]
    }
   ],
   "source": [
    "# Build directed citation graph using NetworkX\n",
    "# Edge (i -> j) means paper i cites paper j\n",
    "print('Building directed graph...')\n",
    "start = time.time()\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(range(n))\n",
    "\n",
    "for i, refs in enumerate(refs_by_id):\n",
    "    for ref_id in refs:\n",
    "        j = id_to_idx.get(ref_id)\n",
    "        if j is not None:\n",
    "            G.add_edge(i, j)\n",
    "\n",
    "print(f'Graph built in {time.time()-start:.2f}s')\n",
    "print(f'Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a450143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function defined\n"
     ]
    }
   ],
   "source": [
    "# Function to compute PageRank and correlations\n",
    "def compute_pagerank_and_correlation(G, alpha, k=50):\n",
    "    \"\"\"\n",
    "    Compute PageRank with damping factor alpha.\n",
    "    Select top-k papers by PageRank.\n",
    "    Compute in-degree (citation count) for top-k papers.\n",
    "    Return Pearson correlation between PageRank and in-degree.\n",
    "    \"\"\"\n",
    "    # Compute PageRank\n",
    "    pr = nx.pagerank(G, alpha=alpha)\n",
    "    \n",
    "    # Get top-k papers by PageRank\n",
    "    top_k_papers = sorted(pr.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    top_k_indices = [idx for idx, _ in top_k_papers]\n",
    "    top_k_pr_scores = [score for _, score in top_k_papers]\n",
    "    \n",
    "    # Compute in-degree (citation count) for top-k papers\n",
    "    in_degrees = [G.in_degree(idx) for idx in top_k_indices]\n",
    "    \n",
    "    # Compute Pearson correlation\n",
    "    if len(top_k_pr_scores) > 1 and np.std(in_degrees) > 0:\n",
    "        corr, _ = pearsonr(top_k_pr_scores, in_degrees)\n",
    "    else:\n",
    "        corr = 0.0\n",
    "    \n",
    "    return pr, top_k_papers, top_k_indices, in_degrees, corr\n",
    "\n",
    "print('Function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3ffa76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Damping factors: [0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95]\n",
      "\n",
      "Computing PageRank with alpha=0.15...\n",
      "  Correlation: 0.7858 (computed in 0.12s)\n",
      "\n",
      "Computing PageRank with alpha=0.25...\n",
      "  Correlation: 0.7688 (computed in 0.19s)\n",
      "\n",
      "Computing PageRank with alpha=0.35...\n",
      "  Correlation: 0.7457 (computed in 0.18s)\n",
      "\n",
      "Computing PageRank with alpha=0.45...\n",
      "  Correlation: 0.7323 (computed in 0.10s)\n",
      "\n",
      "Computing PageRank with alpha=0.55...\n",
      "  Correlation: 0.7128 (computed in 0.10s)\n",
      "\n",
      "Computing PageRank with alpha=0.65...\n",
      "  Correlation: 0.6889 (computed in 0.10s)\n",
      "\n",
      "Computing PageRank with alpha=0.75...\n",
      "  Correlation: 0.6742 (computed in 0.10s)\n",
      "\n",
      "Computing PageRank with alpha=0.85...\n",
      "  Correlation: 0.5542 (computed in 0.10s)\n",
      "\n",
      "Computing PageRank with alpha=0.95...\n",
      "  Correlation: 0.6400 (computed in 0.10s)\n"
     ]
    }
   ],
   "source": [
    "# Compute PageRank and correlations for all damping factors\n",
    "damping_factors = np.arange(0.15, 1.0, 0.10)\n",
    "damping_factors = np.round(damping_factors, 2)\n",
    "\n",
    "print('Damping factors:', damping_factors)\n",
    "\n",
    "results = {}\n",
    "k = 50\n",
    "\n",
    "for alpha in damping_factors:\n",
    "    print(f'\\nComputing PageRank with alpha={alpha}...')\n",
    "    start = time.time()\n",
    "    pr, top_k_papers, top_k_indices, in_degrees, corr = compute_pagerank_and_correlation(G, alpha, k)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    results[alpha] = {\n",
    "        'pagerank': pr,\n",
    "        'top_k_papers': top_k_papers,\n",
    "        'top_k_indices': top_k_indices,\n",
    "        'in_degrees': in_degrees,\n",
    "        'correlation': corr\n",
    "    }\n",
    "    print(f'  Correlation: {corr:.4f} (computed in {elapsed:.2f}s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b03274f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CORRELATION VALUES FOR EACH DAMPING FACTOR\n",
      "======================================================================\n",
      "Damping Factor       Pearson Correlation \n",
      "----------------------------------------------------------------------\n",
      "0.15                 0.785755            \n",
      "0.25                 0.768759            \n",
      "0.35                 0.745674            \n",
      "0.45                 0.732333            \n",
      "0.55                 0.712790            \n",
      "0.65                 0.688880            \n",
      "0.75                 0.674215            \n",
      "0.85                 0.554154            \n",
      "0.95                 0.640048            \n"
     ]
    }
   ],
   "source": [
    "# Report: Correlation values for all damping factors\n",
    "print('\\n' + '='*70)\n",
    "print('CORRELATION VALUES FOR EACH DAMPING FACTOR')\n",
    "print('='*70)\n",
    "print(f'{\"Damping Factor\":<20} {\"Pearson Correlation\":<20}')\n",
    "print('-'*70)\n",
    "\n",
    "correlations_dict = {}\n",
    "for alpha in damping_factors:\n",
    "    corr = results[alpha]['correlation']\n",
    "    correlations_dict[alpha] = corr\n",
    "    print(f'{alpha:<20.2f} {corr:<20.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87171843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best correlation: alpha=0.15, correlation=0.785755\n",
      "Worst correlation: alpha=0.85, correlation=0.554154\n"
     ]
    }
   ],
   "source": [
    "# Find best and worst correlation values\n",
    "best_alpha = max(correlations_dict, key=correlations_dict.get)\n",
    "worst_alpha = min(correlations_dict, key=correlations_dict.get)\n",
    "\n",
    "best_corr = correlations_dict[best_alpha]\n",
    "worst_corr = correlations_dict[worst_alpha]\n",
    "\n",
    "print(f'\\nBest correlation: alpha={best_alpha}, correlation={best_corr:.6f}')\n",
    "print(f'Worst correlation: alpha={worst_alpha}, correlation={worst_corr:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a982233a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TOP-10 PAPERS FOR BEST CORRELATION (alpha=0.15)\n",
      "======================================================================\n",
      "S.No.  Title                                    PageRank Score      \n",
      "----------------------------------------------------------------------\n",
      "1      LIBSVM: A library for support vector ... 0.00084804          \n",
      "2      The Pascal Visual Object Classes (VOC... 0.00027491          \n",
      "3      Object Detection with Discriminativel... 0.00022282          \n",
      "4      Community detection in graphs            0.00016004          \n",
      "5      Fast and Scalable Local Kernel Machines  0.00014622          \n",
      "6      What is Twitter, a social network or ... 0.00014267          \n",
      "7      Reducibility Among Combinatorial Prob... 0.00013088          \n",
      "8      Talking about tactile experiences        0.00011013          \n",
      "9      ImageNet Classification with Deep Con... 0.00010258          \n",
      "10     KEGG for representation and analysis ... 0.00009980          \n"
     ]
    }
   ],
   "source": [
    "# Report: Top-10 papers for BEST correlation\n",
    "print('\\n' + '='*70)\n",
    "print(f'TOP-10 PAPERS FOR BEST CORRELATION (alpha={best_alpha})')\n",
    "print('='*70)\n",
    "print(f'{\"S.No.\":<6} {\"Title\":<40} {\"PageRank Score\":<20}')\n",
    "print('-'*70)\n",
    "\n",
    "best_top_k = results[best_alpha]['top_k_papers'][:10]\n",
    "for rank, (idx, pr_score) in enumerate(best_top_k, start=1):\n",
    "    title = titles[idx]\n",
    "    # Truncate title if too long\n",
    "    if len(title) > 40:\n",
    "        title = title[:37] + '...'\n",
    "    print(f'{rank:<6} {title:<40} {pr_score:<20.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e41e5234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TOP-10 PAPERS FOR WORST CORRELATION (alpha=0.85)\n",
      "======================================================================\n",
      "S.No.  Title                                    PageRank Score      \n",
      "----------------------------------------------------------------------\n",
      "1      LIBSVM: A library for support vector ... 0.00892469          \n",
      "2      Fast and Scalable Local Kernel Machines  0.00727250          \n",
      "3      The Pascal Visual Object Classes (VOC... 0.00448177          \n",
      "4      Factored Shapes and Appearances for P... 0.00390998          \n",
      "5      Object Detection with Discriminativel... 0.00262820          \n",
      "6      ClassCut for unsupervised class segme... 0.00219337          \n",
      "7      Community detection in graphs            0.00101820          \n",
      "8      A Singular Value Thresholding Algorit... 0.00095931          \n",
      "9      TwitterRank: finding topic-sensitive ... 0.00094809          \n",
      "10     Guaranteed Minimum-Rank Solutions of ... 0.00090303          \n"
     ]
    }
   ],
   "source": [
    "# Report: Top-10 papers for WORST correlation\n",
    "print('\\n' + '='*70)\n",
    "print(f'TOP-10 PAPERS FOR WORST CORRELATION (alpha={worst_alpha})')\n",
    "print('='*70)\n",
    "print(f'{\"S.No.\":<6} {\"Title\":<40} {\"PageRank Score\":<20}')\n",
    "print('-'*70)\n",
    "\n",
    "worst_top_k = results[worst_alpha]['top_k_papers'][:10]\n",
    "for rank, (idx, pr_score) in enumerate(worst_top_k, start=1):\n",
    "    title = titles[idx]\n",
    "    # Truncate title if too long\n",
    "    if len(title) > 40:\n",
    "        title = title[:37] + '...'\n",
    "    print(f'{rank:<6} {title:<40} {pr_score:<20.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c5e2bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved correlation table to /Users/ankushchhabra/Downloads/Data Mining Assignment2/pagerank_correlations.csv\n",
      "Saved best top-10 to /Users/ankushchhabra/Downloads/Data Mining Assignment2/pagerank_best_top10.csv\n",
      "Saved worst top-10 to /Users/ankushchhabra/Downloads/Data Mining Assignment2/pagerank_worst_top10.csv\n"
     ]
    }
   ],
   "source": [
    "# Save results to CSV for easy reference\n",
    "import csv\n",
    "\n",
    "# Save correlation table\n",
    "corr_csv_path = os.path.join(os.getcwd(), 'pagerank_correlations.csv')\n",
    "with open(corr_csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['Damping_Factor', 'Pearson_Correlation'])\n",
    "    for alpha in damping_factors:\n",
    "        w.writerow([f'{alpha:.2f}', f'{correlations_dict[alpha]:.6f}'])\n",
    "\n",
    "print(f'Saved correlation table to {corr_csv_path}')\n",
    "\n",
    "# Save best top-10\n",
    "best_csv_path = os.path.join(os.getcwd(), 'pagerank_best_top10.csv')\n",
    "with open(best_csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['Rank', 'Paper_Index', 'Title', 'PageRank_Score'])\n",
    "    for rank, (idx, pr_score) in enumerate(best_top_k, start=1):\n",
    "        w.writerow([rank, idx, titles[idx], f'{pr_score:.8f}'])\n",
    "\n",
    "print(f'Saved best top-10 to {best_csv_path}')\n",
    "\n",
    "# Save worst top-10\n",
    "worst_csv_path = os.path.join(os.getcwd(), 'pagerank_worst_top10.csv')\n",
    "with open(worst_csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['Rank', 'Paper_Index', 'Title', 'PageRank_Score'])\n",
    "    for rank, (idx, pr_score) in enumerate(worst_top_k, start=1):\n",
    "        w.writerow([rank, idx, titles[idx], f'{pr_score:.8f}'])\n",
    "\n",
    "print(f'Saved worst top-10 to {worst_csv_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b0853a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook computes PageRank scores for the citation network using various damping factors and measures the correlation between PageRank and citation counts (in-degree) for the top-50 papers.\n",
    "\n",
    "Key insights:\n",
    "- The damping factor affects how PageRank distributes authority through the network.\n",
    "- Lower damping factors (0.15–0.35) give more weight to direct links (in-degree).\n",
    "- Higher damping factors (0.75–0.95) give more weight to indirect paths and overall network structure.\n",
    "- The correlation shows whether PageRank aligns with simple citation counts; higher correlations suggest PageRank is capturing similar ranking signals.\n",
    "\n",
    "Results are saved to CSV files:\n",
    "- `pagerank_correlations.csv` — all damping factors and correlations\n",
    "- `pagerank_best_top10.csv` — top-10 papers for best correlation\n",
    "- `pagerank_worst_top10.csv` — top-10 papers for worst correlation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
